name: Hardware Tests

on:
  schedule:
    # Run hardware tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      hardware_type:
        description: 'Hardware to test'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - loihi2
          - spinnaker
          - gpu
      test_type:
        description: 'Test type'
        required: true
        default: 'integration'
        type: choice
        options:
          - integration
          - performance
          - both

env:
  PYTHON_VERSION: '3.11'

jobs:
  # GPU Tests (Available on GitHub runners)
  gpu-tests:
    name: GPU Integration Tests
    runs-on: ubuntu-latest-gpu  # Self-hosted runner with GPU
    timeout-minutes: 60
    if: |
      (github.event_name == 'schedule') ||
      (github.event.inputs.hardware_type == 'all') ||
      (github.event.inputs.hardware_type == 'gpu')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Check GPU availability
      run: |
        nvidia-smi
        python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
        python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

    - name: Run GPU integration tests
      run: |
        pytest tests/hardware/test_gpu.py -v --gpu --tb=short
      env:
        CUDA_VISIBLE_DEVICES: "0"

    - name: Run GPU performance tests
      if: |
        (github.event.inputs.test_type == 'performance') ||
        (github.event.inputs.test_type == 'both') ||
        (github.event_name == 'schedule')
      run: |
        pytest tests/performance/ -v --gpu --benchmark-json=gpu-benchmark.json
      env:
        CUDA_VISIBLE_DEVICES: "0"

    - name: Upload GPU benchmark results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: gpu-benchmark-results
        path: gpu-benchmark.json
        retention-days: 30

  # Loihi 2 Tests (Self-hosted runner required)
  loihi2-tests:
    name: Loihi 2 Hardware Tests
    runs-on: [self-hosted, loihi2]
    timeout-minutes: 90
    if: |
      (github.event_name == 'schedule') ||
      (github.event.inputs.hardware_type == 'all') ||
      (github.event.inputs.hardware_type == 'loihi2')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Check Loihi 2 availability
      run: |
        python -c "
        try:
            import nxsdk
            print('âœ… NxSDK available')
            print(f'NxSDK version: {nxsdk.__version__}')
        except ImportError:
            print('âŒ NxSDK not available')
            exit(1)
        "

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[loihi2,dev]"

    - name: Validate hardware configurations
      run: |
        python scripts/validate_hardware_configs.py --hardware loihi2

    - name: Run Loihi 2 integration tests
      run: |
        pytest tests/hardware/test_loihi2.py -v --loihi2 --tb=short
      env:
        PYTHONPATH: $NXSDK_DIR/lib:$PYTHONPATH
        KAPOHOBAY: 1

    - name: Run Loihi 2 performance tests
      if: |
        (github.event.inputs.test_type == 'performance') ||
        (github.event.inputs.test_type == 'both') ||
        (github.event_name == 'schedule')
      run: |
        pytest tests/performance/ -v --loihi2 --benchmark-json=loihi2-benchmark.json
      env:
        PYTHONPATH: $NXSDK_DIR/lib:$PYTHONPATH
        KAPOHOBAY: 1

    - name: Generate hardware report
      if: always()
      run: |
        python scripts/generate_hardware_report.py --hardware loihi2 --output loihi2-report.json

    - name: Upload Loihi 2 results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: loihi2-test-results
        path: |
          loihi2-benchmark.json
          loihi2-report.json
        retention-days: 30

  # SpiNNaker Tests (Self-hosted runner required)
  spinnaker-tests:
    name: SpiNNaker Hardware Tests
    runs-on: [self-hosted, spinnaker]
    timeout-minutes: 90
    if: |
      (github.event_name == 'schedule') ||
      (github.event.inputs.hardware_type == 'all') ||
      (github.event.inputs.hardware_type == 'spinnaker')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Check SpiNNaker availability
      run: |
        python -c "
        try:
            import spynnaker
            print('âœ… sPyNNaker available')
            print(f'sPyNNaker version: {spynnaker.__version__}')
        except ImportError:
            print('âŒ sPyNNaker not available')
            exit(1)
        "

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[spinnaker,dev]"

    - name: Configure SpiNNaker
      run: |
        # Set up SpiNNaker configuration
        python -c "
        import spynnaker_cfg
        cfg = spynnaker_cfg.create_default_cfg()
        cfg.set('Machine', 'machine_name', 'spinn-4.cs.man.ac.uk')
        cfg.set('Machine', 'version', 5)
        with open('.spynnaker.cfg', 'w') as f:
            cfg.write(f)
        "

    - name: Run SpiNNaker integration tests
      run: |
        pytest tests/hardware/test_spinnaker.py -v --spinnaker --tb=short
      env:
        SPYNNAKER_CFG: ".spynnaker.cfg"

    - name: Run SpiNNaker performance tests
      if: |
        (github.event.inputs.test_type == 'performance') ||
        (github.event.inputs.test_type == 'both') ||
        (github.event_name == 'schedule')
      run: |
        pytest tests/performance/ -v --spinnaker --benchmark-json=spinnaker-benchmark.json
      env:
        SPYNNAKER_CFG: ".spynnaker.cfg"

    - name: Generate hardware report
      if: always()
      run: |
        python scripts/generate_hardware_report.py --hardware spinnaker --output spinnaker-report.json

    - name: Upload SpiNNaker results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: spinnaker-test-results
        path: |
          spinnaker-benchmark.json
          spinnaker-report.json
        retention-days: 30

  # Energy Profiling Tests
  energy-profiling:
    name: Energy Profiling Tests
    runs-on: [self-hosted, energy-monitor]
    timeout-minutes: 45
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[all,dev]"

    - name: Run energy profiling tests
      run: |
        python scripts/energy_profiling.py --all-backends --output energy-profile.json

    - name: Generate energy comparison report
      run: |
        python scripts/generate_energy_report.py --input energy-profile.json --output energy-report.html

    - name: Upload energy profiling results
      uses: actions/upload-artifact@v4
      with:
        name: energy-profiling-results
        path: |
          energy-profile.json
          energy-report.html
        retention-days: 90

  # Consolidate Results
  hardware-test-summary:
    name: Hardware Test Summary
    runs-on: ubuntu-latest
    needs: [gpu-tests, loihi2-tests, spinnaker-tests, energy-profiling]
    if: always()
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: hardware-results/

    - name: Generate comprehensive report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'test_run': '${{ github.run_id }}',
            'results': {}
        }
        
        # Collect all JSON results
        for root, dirs, files in os.walk('hardware-results'):
            for file in files:
                if file.endswith('.json'):
                    with open(os.path.join(root, file)) as f:
                        try:
                            data = json.load(f)
                            report['results'][file] = data
                        except:
                            pass
        
        with open('hardware-test-summary.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f'ðŸ“Š Hardware test summary generated with {len(report[\"results\"])} result files')
        "

    - name: Upload consolidated report
      uses: actions/upload-artifact@v4
      with:
        name: hardware-test-summary
        path: hardware-test-summary.json
        retention-days: 90

    - name: Check test status
      run: |
        echo "## Hardware Test Results" > test-summary.md
        echo "" >> test-summary.md
        
        if [[ "${{ needs.gpu-tests.result }}" == "success" ]]; then
          echo "âœ… GPU Tests: PASSED" >> test-summary.md
        else
          echo "âŒ GPU Tests: ${{ needs.gpu-tests.result }}" >> test-summary.md
        fi
        
        if [[ "${{ needs.loihi2-tests.result }}" == "success" ]]; then
          echo "âœ… Loihi 2 Tests: PASSED" >> test-summary.md
        else
          echo "âŒ Loihi 2 Tests: ${{ needs.loihi2-tests.result }}" >> test-summary.md
        fi
        
        if [[ "${{ needs.spinnaker-tests.result }}" == "success" ]]; then
          echo "âœ… SpiNNaker Tests: PASSED" >> test-summary.md
        else
          echo "âŒ SpiNNaker Tests: ${{ needs.spinnaker-tests.result }}" >> test-summary.md
        fi
        
        if [[ "${{ needs.energy-profiling.result }}" == "success" ]]; then
          echo "âœ… Energy Profiling: PASSED" >> test-summary.md
        else
          echo "âŒ Energy Profiling: ${{ needs.energy-profiling.result }}" >> test-summary.md
        fi
        
        cat test-summary.md

  # Notify on Failure
  notify-failure:
    name: Notify Hardware Test Failures
    runs-on: ubuntu-latest
    needs: [hardware-test-summary]
    if: failure()
    
    steps:
    - name: Create issue on hardware test failure
      uses: actions/github-script@v7
      with:
        script: |
          const title = `Hardware Tests Failed - Run #${{ github.run_number }}`;
          const body = `
          ## Hardware Test Failure Report
          
          **Run ID**: ${{ github.run_id }}
          **Timestamp**: ${new Date().toISOString()}
          **Workflow**: ${{ github.workflow }}
          
          ### Failed Tests
          - GPU Tests: ${{ needs.gpu-tests.result }}
          - Loihi 2 Tests: ${{ needs.loihi2-tests.result }}
          - SpiNNaker Tests: ${{ needs.spinnaker-tests.result }}
          - Energy Profiling: ${{ needs.energy-profiling.result }}
          
          ### Actions Required
          - [ ] Review test logs in [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - [ ] Check hardware availability and configuration
          - [ ] Update hardware dependencies if needed
          - [ ] Verify test environments are properly configured
          
          **Workflow URL**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          `;
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['bug', 'hardware', 'ci-failure']
          });