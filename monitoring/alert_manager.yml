# AlertManager Configuration for Spikeformer Monitoring
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@your-org.com'
  smtp_auth_username: 'alerts@your-org.com'
  smtp_auth_password_file: '/etc/alertmanager/smtp_password'

# Routing tree for alert distribution  
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default-receiver'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
    group_wait: 5s
    group_interval: 5s
    repeat_interval: 15m
    
  - match:
      severity: warning
    receiver: 'warning-alerts'
    group_interval: 30s
    repeat_interval: 4h
    
  - match:
      alertname: ModelInferenceLatencyHigh
    receiver: 'performance-team'
    
  - match:
      alertname: HardwareTemperatureHigh
    receiver: 'hardware-team'
    
  - match_re:
      alertname: (EnergyConsumption.*|PowerDraw.*)
    receiver: 'energy-team'

# Alert receivers configuration
receivers:
- name: 'default-receiver'
  email_configs:
  - to: 'devops@your-org.com'
    subject: '[Spikeformer] Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Severity: {{ .Labels.severity }}
      Instance: {{ .Labels.instance }}
      Time: {{ .StartsAt }}
      {{ end }}

- name: 'critical-alerts'
  email_configs:
  - to: 'oncall@your-org.com'
    subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
    body: |
      CRITICAL ALERT - Immediate Action Required
      
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Instance: {{ .Labels.instance }}
      Severity: {{ .Labels.severity }}
      Started: {{ .StartsAt }}
      
      Runbook: {{ .Annotations.runbook_url }}
      Dashboard: {{ .Annotations.dashboard_url }}
      {{ end }}
  
  slack_configs:
  - api_url_file: '/etc/alertmanager/slack_webhook'
    channel: '#alerts-critical'
    color: 'danger'
    title: 'üö® Critical Alert: {{ .GroupLabels.alertname }}'
    text: |
      {{ range .Alerts }}
      *Alert:* {{ .Annotations.summary }}
      *Description:* {{ .Annotations.description }}
      *Instance:* {{ .Labels.instance }}
      *Started:* {{ .StartsAt }}
      {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
      {{ end }}

- name: 'warning-alerts'
  email_configs:
  - to: 'team@your-org.com'
    subject: '‚ö†Ô∏è Warning: {{ .GroupLabels.alertname }}'
    
  slack_configs:
  - api_url_file: '/etc/alertmanager/slack_webhook'
    channel: '#alerts-warning'
    color: 'warning'
    title: '‚ö†Ô∏è Warning: {{ .GroupLabels.alertname }}'

- name: 'performance-team'
  email_configs:
  - to: 'performance@your-org.com'
    subject: '[Performance] {{ .GroupLabels.alertname }}'
    
- name: 'hardware-team'
  email_configs:
  - to: 'hardware@your-org.com'
    subject: '[Hardware] {{ .GroupLabels.alertname }}'
    
- name: 'energy-team'
  email_configs:
  - to: 'energy-efficiency@your-org.com'
    subject: '[Energy] {{ .GroupLabels.alertname }}'

# Inhibition rules to prevent alert spam
inhibit_rules:
# Inhibit any warning-level alerts if critical alerts are firing
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'instance']

# Inhibit model latency warnings if model is down
- source_match:
    alertname: 'ModelServiceDown'
  target_match_re:
    alertname: '(ModelInferenceLatency.*|ModelThroughput.*)'
  equal: ['instance']

# Inhibit hardware warnings if hardware is critically failing
- source_match:
    severity: 'critical'
    component: 'hardware'
  target_match:
    severity: 'warning'
    component: 'hardware'
  equal: ['instance']

# Templates for notification formatting
templates:
- '/etc/alertmanager/templates/*.tmpl'