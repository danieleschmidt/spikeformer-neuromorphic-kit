# OpenTelemetry Collector Configuration for Spikeformer
# Comprehensive observability pipeline for neuromorphic AI workloads

receivers:
  # Application traces and metrics
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  
  # Prometheus metrics scraping
  prometheus:
    config:
      scrape_configs:
        - job_name: 'spikeformer-otel'
          static_configs:
            - targets: ['spikeformer-dev:8000', 'spikeformer-cpu:8000', 'spikeformer-gpu:8000']
          metrics_path: '/metrics'
          scrape_interval: 15s
        
        - job_name: 'neuromorphic-hardware'
          static_configs:
            - targets: ['spikeformer-loihi2:8000', 'spikeformer-spinnaker:8000']
          metrics_path: '/hardware-metrics'
          scrape_interval: 5s
  
  # Jaeger traces
  jaeger:
    protocols:
      grpc:
        endpoint: 0.0.0.0:14250
      thrift_http:
        endpoint: 0.0.0.0:14268
      thrift_compact:
        endpoint: 0.0.0.0:6831
      thrift_binary:
        endpoint: 0.0.0.0:6832
  
  # Zipkin traces
  zipkin:
    endpoint: 0.0.0.0:9411
  
  # Host metrics
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
      memory:
        metrics:
          system.memory.utilization:
            enabled: true
      disk:
      filesystem:
        exclude_mount_points:
          mount_points: ["/dev/*", "/proc/*", "/sys/*", "/var/lib/docker/*"]
          match_type: regexp
      network:
      load:
      processes:
  
  # Docker container metrics
  docker_stats:
    endpoint: unix:///var/run/docker.sock
    collection_interval: 10s
    timeout: 5s
    api_version: 1.40

processors:
  # Memory limiter to prevent OOM
  memory_limiter:
    limit_mib: 512
    spike_limit_mib: 128
    check_interval: 5s
  
  # Batch processing for efficiency
  batch:
    timeout: 10s
    send_batch_size: 1024
    send_batch_max_size: 2048
  
  # Resource detection
  resourcedetection:
    detectors: [env, system, docker]
    timeout: 5s
  
  # Attribute processing for neuromorphic-specific metrics
  attributes:
    actions:
      - key: service.name
        action: upsert
        value: spikeformer
      - key: deployment.environment
        from_attribute: NEUROMORPHIC_ENV
        action: upsert
      - key: hardware.type
        from_attribute: HARDWARE_BACKEND
        action: upsert
  
  # Filter processor for reducing noise
  filter/neuromorphic:
    metrics:
      exclude:
        match_type: regexp
        metric_names:
          - ".*debug.*"
          - ".*test.*"
    traces:
      exclude:
        match_type: strict
        services: ["health-check", "metrics-scraper"]
  
  # Transform processor for neuromorphic-specific transformations
  transform:
    metric_statements:
      - context: metric
        statements:
          # Convert spike rates from Hz to kHz for better visualization
          - set(name, "neuromorphic.spike_rate_khz") where name == "neuromorphic.spike_rate_hz"
          - set(value, value / 1000) where name == "neuromorphic.spike_rate_khz"
          
          # Add energy efficiency ratio
          - set(name, "neuromorphic.energy_efficiency_ratio") where name == "neuromorphic.energy_consumption"
          - set(value, 1000 / value) where name == "neuromorphic.energy_efficiency_ratio"
    
    trace_statements:
      - context: span
        statements:
          # Add neuromorphic-specific span attributes
          - set(attributes["neuromorphic.model_conversion"], "true") where name == "convert_model"
          - set(attributes["neuromorphic.hardware_deployment"], "true") where name == "deploy_to_hardware"

exporters:
  # Prometheus metrics export
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: "spikeformer"
    const_labels:
      deployment: "neuromorphic"
    
  # Jaeger traces export
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true
  
  # OTLP export to multiple backends
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true
  
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true
  
  # Elasticsearch for logs and traces
  elasticsearch:
    endpoints: ["http://elasticsearch:9200"]
    index: "spikeformer-traces"
    pipeline: "spikeformer-pipeline"
  
  # File export for debugging
  file:
    path: "/tmp/otel-output.json"
    rotation:
      max_megabytes: 100
      max_days: 7
      max_backups: 3
  
  # Cloud exports (commented out - configure as needed)
  # googlegcloud:
  #   project: "your-gcp-project"
  #   trace:
  #     endpoint: "cloudtrace.googleapis.com:443"
  # 
  # azuremonitor:
  #   instrumentation_key: "your-azure-key"
  # 
  # awsxray:
  #   region: "us-west-2"

extensions:
  # Health check endpoint
  health_check:
    endpoint: 0.0.0.0:13133
  
  # Performance profiler
  pprof:
    endpoint: 0.0.0.0:1777
  
  # zPages for debugging
  zpages:
    endpoint: 0.0.0.0:55679

service:
  extensions: [health_check, pprof, zpages]
  
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp, jaeger, zipkin]
      processors: [memory_limiter, resourcedetection, attributes, batch, transform]
      exporters: [jaeger, otlp/jaeger, otlp/tempo]
    
    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus, hostmetrics, docker_stats]
      processors: [memory_limiter, resourcedetection, attributes, filter/neuromorphic, batch, transform]
      exporters: [prometheus]
    
    # Logs pipeline (if needed)
    logs:
      receivers: [otlp]
      processors: [memory_limiter, resourcedetection, attributes, batch]
      exporters: [elasticsearch, file]
  
  # Telemetry configuration
  telemetry:
    logs:
      level: "info"
      development: false
      sampling:
        enabled: true
        tick: 10s
        initial: 5
        thereafter: 1000
    
    metrics:
      level: detailed
      address: 0.0.0.0:8888
    
    traces:
      processors: [batch]